{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0174ca-1b94-4f2a-b426-c1ecf79414a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,List\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ea246d-dca0-4883-98bd-2f2f4e67548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(tfk.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions:int,\n",
    "        emb_dim:Optional[int]=100,\n",
    "        hidden_units:Optional[int]=200,\n",
    "        seq_len:Optional[int]=10,\n",
    "        batch_size:Optional[int]=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._num_actions = num_actions\n",
    "        self._emb_dim = emb_dim\n",
    "        self._hidden_units = hidden_units\n",
    "        self._seq_len = seq_len\n",
    "        \n",
    "        self._dropout = tfk.layers.Dropout(0.5)\n",
    "        self._emb_layer = tfk.layers.Embedding(self._num_actions, self._emb_dim)\n",
    "        self._gru_layer = tfk.layers.GRU(self._hidden_units, input_shape=(self._seq_len, self._emb_dim))\n",
    "        self._linear1 = tfk.layers.Dense(self._num_actions)\n",
    "        #self._linear2 = tfk.layers.Dense(self._num_actions)\n",
    "        \n",
    "        \n",
    "        dummy_state = tf.zeros((batch_size, seq_len), dtype=tf.int32)\n",
    "        self(dummy_state)\n",
    "    def call(self, state):\n",
    "        x = self._emb_layer(state)\n",
    "        x = self._gru_layer(x)\n",
    "        x = self._dropout(x)\n",
    "        score = self._linear1(x)\n",
    "        #q = self._linear2(x)\n",
    "\n",
    "        #return score, q\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f137a-e4c8-490c-b382-36706a16caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77137b59-5a40-489d-95fc-9e8318659ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions:int,\n",
    "        emb_dim:Optional[int]=100,\n",
    "        hidden_units:Optional[int]=200,\n",
    "        seq_len:Optional[int]=10,\n",
    "        gamma:Optional[float]=1.,\n",
    "        lr:Optional[float]=0.01,\n",
    "        batch_size:Optional[int]=256\n",
    "    ):\n",
    "        self._num_actions = num_actions\n",
    "        self._emb_dim = emb_dim\n",
    "        self._hidden_units = hidden_units\n",
    "        self._seq_len = seq_len\n",
    "        #self._gamma = gamma\n",
    "        \n",
    "        self._qnet = QNet(num_actions, emb_dim, hidden_units, seq_len, batch_size)\n",
    "        #self._target_qnet = copy.deepcopy(self._qnet)\n",
    "        \n",
    "        #self._tdloss = tfk.losses.Huber()\n",
    "        self._loss = tfk.losses.SparseCategoricalCrossentropy()\n",
    "        self._optim = tfk.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "\n",
    "    def compute_target_sa_values(self, state, action):\n",
    "        _, q_values = self._target_qnet(state)\n",
    "        action_one_hot = tf.one_hot(action, depth=self._num_actions)\n",
    "        return tf.reduce_sum(q_values*action_one_hot, axis=1)\n",
    "\n",
    "    def compute_predict_sa_values(self, state, action):\n",
    "        _, q_values = self._qnet(state)\n",
    "        action_one_hot = tf.one_hot(action, depth=self._num_actions)\n",
    "        return tf.reduce_sum(q_values*action_one_hot, axis=1)\n",
    "    \n",
    "    def update_params(self, tau=0.9):\n",
    "        for param, tar_param in zip(self._qnet.trainable_variables, self._target_qnet.trainable_variables):\n",
    "            tar_param.assign(param*tau + (1-tau)*tar_param)\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(self, batch):\n",
    "        #_, q_target = self._target_qnet(batch[\"n_state\"])\n",
    "        #q_target = batch[\"reward\"] + self._gamma * self.compute_target_sa_values(batch[\"n_state\"], tf.argmax(q_target, axis=1))\n",
    "        #q_target = tf.stop_gradient(q_target)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            #q_pred = self.compute_predict_sa_values(batch[\"state\"], batch[\"action\"])\n",
    "            #score, _ = self._qnet(batch[\"state\"])\n",
    "            #td_loss = self._tdloss(q_target, q_pred)\n",
    "            score = self._qnet(batch[\"state\"])\n",
    "            loss = self._loss(batch[\"action\"], score)\n",
    "            #loss += td_loss\n",
    "        grad = tape.gradient(loss, self._qnet.trainable_variables)\n",
    "        self._optim.apply_gradients(zip(grad, self._qnet.trainable_variables))\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        train_data:tf.data.Dataset,\n",
    "        n_epochs=10,\n",
    "        update_iter=10,\n",
    "        tau=0.9\n",
    "    ):\n",
    "        losses = []\n",
    "        best_loss = np.Inf\n",
    "        stop_count = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            batch_loss = 0.\n",
    "            with tqdm(train_data, desc=\"[Epoch%d]\"%(epoch+1)) as ts:\n",
    "                for i, batch in enumerate(ts):\n",
    "                    loss = self._train_step(batch)\n",
    "                    batch_loss += loss\n",
    "                    ts.set_postfix_str(\"Loss=%4f\"%(batch_loss / (i+1)))\n",
    "                    \n",
    "                    #if (i+1)%update_iter == 0:\n",
    "                        #self.update_params(tau)\n",
    "                batch_loss /= (i+1)\n",
    "\n",
    "            if batch_loss >= best_loss:\n",
    "                stop_count += 1\n",
    "            else:\n",
    "                best_loss = batch_loss\n",
    "                stop_count = 0\n",
    "                \n",
    "            if stop_count > 3:\n",
    "                break \n",
    "            losses += [batch_loss.numpy()]\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2541212-d848-4bdd-824a-02db81ac0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mdp = pickle.load(open(\"/home/inoue/work/dataset/diginetica2/derived/mdp_train.df\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92a5b71-991f-47c7-b9cc-a03a5f4ad686",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    {\n",
    "        \"sess\" : train_mdp[0].astype(np.int32),\n",
    "        \"state\":train_mdp[1].astype(np.int32),\n",
    "        \"action\":train_mdp[2].astype(np.int32),\n",
    "        \"reward\":train_mdp[3].astype(np.float32),\n",
    "        \"n_state\":train_mdp[4].astype(np.int32),\n",
    "        \"done\":train_mdp[5].astype(np.float32)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2390f6f-52de-4c06-b8a2-3907d8b0168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(42171, seq_len=3, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77eb36f-91e8-494f-90d7-e818ddb7450c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b16370379b473795c2722de24dcf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch1]:   0%|          | 0/718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74b6a2df981458dadf48ee07381dde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch2]:   0%|          | 0/718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaf83ee7c3d4777b49d9f62cc841854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch3]:   0%|          | 0/718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f893456c2e974180a8783b4fe059cd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch4]:   0%|          | 0/718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae6a0e0df7243238d63656ea8d0ca9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch5]:   0%|          | 0/718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383b1208162241f8b3e6cba8c347d9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch6]:   0%|          | 0/718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca28c5aaf283448ea162ce55afef7a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch7]:   0%|          | 0/718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = model.fit(train_data.batch(500),n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b00eb0d-44ea-45a3-8bfa-c529b581c9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=10.697142>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.6494465>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.649439>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.649439>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.649439>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.649439>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7cb16-736d-4403-8b49-953a7db7afc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
