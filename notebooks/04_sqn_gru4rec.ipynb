{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254dc8ff-99d8-4793-b297-57c29d7be3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f391f60-e8e2-4056-8367-a4feaacbddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6b8e06-d555-4c70-8ac9-0177e288d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,List\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from recs.dataset import session_parallel_dataset\n",
    "from recs.evaluator import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae544dac-e5de-4634-9c67-9bbf4ab58800",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensorflowによる実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3556c4-8317-420c-99b0-9719f96ec347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQNGRUModel(tfk.layers.Layer):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_items:int,\n",
    "        seq_len:Optional[int]=3,\n",
    "        hidden_dim:Optional[int]=100,\n",
    "        embed_dim:Optional[int]=100,\n",
    "        dropout_rate:Optional[float]=0.5,\n",
    "        name=\"GRU\"\n",
    "    ):\n",
    "        super(SQNGRUModel, self).__init__(name=name)\n",
    "        \n",
    "        \n",
    "        self._embedding = tfk.layers.Embedding(num_items, embed_dim, mask_zero=True)\n",
    "        self._gru = tfk.layers.GRU(\n",
    "            hidden_dim, \n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self._dense = tfk.layers.Dense(num_items, activation=\"softmax\")\n",
    "        self._qvalue_dense = tfk.layers.Dense(num_items, activation=None)\n",
    "    \n",
    "    def call(\n",
    "        self, \n",
    "        item_seqs:tf.Tensor, # (batch_size, seq_len)\n",
    "        training:Optional[bool]=False,\n",
    "        is_next:Optional[bool]=False\n",
    "    ):\n",
    "        x = self._embedding(item_seqs)\n",
    "        x = self._gru(x, training=training)\n",
    "        qvalue = self._qvalue_dense(x)\n",
    "        if is_next:\n",
    "            return qvalue\n",
    "        out = self._dense(x)\n",
    "            \n",
    "        return out, qvalue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7474f9e-3a40-42df-a23a-15ed22a82f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQNGRU4Rec(tfk.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_items:int,\n",
    "        seq_len:Optional[int]=3,\n",
    "        hidden_dim:Optional[int]=100,\n",
    "        embed_dim:Optional[int]=100,\n",
    "        dropout_rate:Optional[float]=0.5,\n",
    "        gamma:Optional[float]=1.,\n",
    "        k:Optional[int]=20,\n",
    "        name=\"SQN-GRUModel\"\n",
    "    ):\n",
    "        super(SQNGRU4Rec, self).__init__(name=name)\n",
    "        self._gamma = gamma\n",
    "        self._num_items = num_items\n",
    "        self._topk = k\n",
    "        \n",
    "        self._gmodel = SQNGRUModel(num_items, seq_len, hidden_dim, embed_dim, dropout_rate, name=\"SQNGRU\")\n",
    "        self._target_gmodel = SQNGRUModel(num_items, seq_len, hidden_dim, embed_dim, dropout_rate, name=\"TargetSQNGRU\")\n",
    "        \n",
    "        self._loss_tracker = tfk.metrics.Mean(name=\"loss\")\n",
    "        self._tdloss_tracker = tfk.metrics.Mean(name=\"TD Error\")\n",
    "        self._recall_tracker = tfk.metrics.Recall(name=\"recall\")\n",
    "            \n",
    "        dummy_state = tf.zeros((1, seq_len), dtype=tf.int32)\n",
    "        self._gmodel(dummy_state)\n",
    "        self._target_gmodel(dummy_state)\n",
    "\n",
    "    \n",
    "    def compile(self, g_loss, q_loss, optimizer):\n",
    "        super(SQNGRU4Rec, self).compile()\n",
    "        self.g_loss = g_loss\n",
    "        self.q_loss = q_loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def call(self, states):\n",
    "        x, _ = self._gmodel(states)\n",
    "        return x\n",
    "    \n",
    "    def __train_step(self, data):\n",
    "        state, action, reward, n_state, done = data\n",
    "        onehot_act = tf.one_hot(action-1, depth=self._num_items)\n",
    "\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            out, qvalue = self._gmodel(state, training=True)\n",
    "            n_qvalue = self._gmodel(n_state, training=True, is_next=True)\n",
    "            n_qvalue_ = self._target_gmodel(n_state, training=True, is_next=True)\n",
    "            \n",
    "            greedy_a = tf.argmax(n_qvalue, axis=-1)\n",
    "            onehot_greedy_a = tf.one_hot(greedy_a, depth=self._num_items)\n",
    "            \n",
    "            Lq = reward + (1.0 - done) * self._gamma * tf.reduce_sum(n_qvalue_*onehot_greedy_a, axis=-1)\n",
    "            Lq = tf.stop_gradient(Lq)\n",
    "            Lq = self.q_loss(Lq, tf.reduce_sum(qvalue*onehot_act,axis=-1))\n",
    "            \n",
    "            Ls = self.g_loss(onehot_act, out)\n",
    "            loss = Lq + Ls\n",
    "            \n",
    "            \n",
    "        grads = tape.gradient(loss, self._gmodel.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self._gmodel.trainable_variables))\n",
    "        self._loss_tracker.update_state(loss)\n",
    "\n",
    "        self._tdloss_tracker.update_state(Lq)\n",
    "        \n",
    "        return {\"loss\": self._loss_tracker.result(), \"TD Error\":self._tdloss_tracker.result()}\n",
    "    \n",
    "    def __tar_train_step(self, data):\n",
    "        state, action, reward, n_state, done = data\n",
    "        onehot_act = tf.one_hot(action-1, depth=self._num_items)\n",
    "\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            out, qvalue = self._target_gmodel(state, training=True)\n",
    "            n_qvalue = self._target_gmodel(n_state, training=True, is_next=True)\n",
    "            n_qvalue_ = self._gmodel(n_state, training=True, is_next=True)\n",
    "            \n",
    "            greedy_a = tf.argmax(n_qvalue, axis=-1)\n",
    "            onehot_greedy_a = tf.one_hot(greedy_a, depth=self._num_items)\n",
    "            \n",
    "            Lq = reward + (1.0 - done) * self._gamma * tf.reduce_sum(n_qvalue_*onehot_greedy_a, axis=-1)\n",
    "            Lq = tf.stop_gradient(Lq)\n",
    "            Lq = self.q_loss(Lq, tf.reduce_sum(qvalue*onehot_act,axis=-1))\n",
    "            \n",
    "            Ls = self.g_loss(onehot_act, out)\n",
    "            loss = Lq + Ls\n",
    "            \n",
    "            \n",
    "        grads = tape.gradient(loss, self._target_gmodel.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self._target_gmodel.trainable_variables))\n",
    "        self._loss_tracker.update_state(loss)\n",
    "\n",
    "        self._tdloss_tracker.update_state(Lq)\n",
    "        \n",
    "        return {\"loss\": self._loss_tracker.result(), \"TD Error\":self._tdloss_tracker.result()}\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        \n",
    "        if np.random.uniform(0, 1) <= 0.5:\n",
    "            loss_hist = self.__train_step(data)\n",
    "        else:\n",
    "            loss_hist = self.__tar_train_step(data)\n",
    "        \n",
    "        return loss_hist   \n",
    "    \n",
    "    def test_step(self, data):\n",
    "        state, target, _, _, _ = data\n",
    "        target = tf.one_hot(target-1, depth=self._num_items)\n",
    "        target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "        qvalue = self(state)\n",
    "        topkitem = tf.math.top_k(qvalue, k=self._topk)[1]\n",
    "        topkitem = tf.reduce_sum(tf.one_hot(topkitem, depth=self._num_items), axis=1)\n",
    "        topkitem = tf.cast(topkitem, dtype=tf.int32)\n",
    "        \n",
    "        self._recall_tracker.update_state(target, topkitem)\n",
    "        \n",
    "        return {\"recall\":self._recall_tracker.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self._loss_tracker, self._recall_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c89a3cc-85bb-4281-8cbf-99103b3a16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname=\"diginetica\"\n",
    "modelname = \"SQNGRU4Rec\"\n",
    "default_logdir = \"/home/inoue/work/recs/\"\n",
    "log_dir =  os.path.join(default_logdir, \"logs/%s/%s/\"%(dataname, modelname)+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "train = pickle.load(open(\n",
    "    \"/home/inoue/work/dataset/%s/derived/mdp_train.df\"%dataname, \"rb\"\n",
    "))\n",
    "\n",
    "split_ind = int(len(train[0])*0.9)\n",
    "data = pd.read_pickle(\"~/work/dataset/%s/derived/train.df\"%dataname)\n",
    "testdata = pd.read_pickle(\"~/work/dataset/%s/derived/test.df\"%dataname)\n",
    "\n",
    "num_items = max(data.itemId.max(), testdata.itemId.max())+1\n",
    "emb_dim = 64\n",
    "hidden_dim = 64\n",
    "seq_len = train[1].shape[1]\n",
    "batch_size=500\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (train[1][:split_ind, :],\n",
    "     train[2][:split_ind],\n",
    "     train[3][:split_ind],\n",
    "     train[4][:split_ind, :], \n",
    "     train[5][:split_ind].astype(np.float32))\n",
    ").shuffle(len(train[0][:split_ind])).batch(batch_size)\n",
    "valid_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (train[1][split_ind:, :],\n",
    "     train[2][split_ind:],\n",
    "     train[3][split_ind:],\n",
    "     train[4][split_ind:, :], \n",
    "     train[5][split_ind:].astype(np.float32))\n",
    ").shuffle(len(train[0][split_ind:])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc70e374-629e-4a85-9d7b-134ddb2bd560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SQNGRU4Rec(\n",
    "    num_items, \n",
    "    seq_len, \n",
    "    hidden_dim, \n",
    "    emb_dim, \n",
    "    dropout_rate=0.1, gamma=0.5)\n",
    "model.compile(\n",
    "    q_loss=tfk.losses.Huber(), \n",
    "    g_loss=tfk.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tfk.optimizers.Adam(learning_rate=0.01)\n",
    ")\n",
    "model.build(input_shape=(1,seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d837afa9-ab05-4310-9f0f-374050ef7b96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "646/646 [==============================] - 53s 70ms/step - loss: 9.3911 - TD Error: 0.2805 - val_recall: 0.2365\n",
      "Epoch 2/100\n",
      "646/646 [==============================] - 45s 68ms/step - loss: 7.1350 - TD Error: 0.3016 - val_recall: 0.3413\n",
      "Epoch 3/100\n",
      "646/646 [==============================] - 44s 67ms/step - loss: 6.0524 - TD Error: 0.3572 - val_recall: 0.3690\n",
      "Epoch 4/100\n",
      "646/646 [==============================] - 44s 67ms/step - loss: 5.3290 - TD Error: 0.4013 - val_recall: 0.3733\n",
      "Epoch 5/100\n",
      "646/646 [==============================] - 43s 65ms/step - loss: 4.8284 - TD Error: 0.4283 - val_recall: 0.3710\n",
      "Epoch 6/100\n",
      "646/646 [==============================] - 43s 64ms/step - loss: 4.4503 - TD Error: 0.4456 - val_recall: 0.3670\n",
      "Epoch 7/100\n",
      "646/646 [==============================] - 42s 64ms/step - loss: 4.1650 - TD Error: 0.4561 - val_recall: 0.3592\n",
      "Epoch 7: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b7c1bc5e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data, \n",
    "    epochs=100, \n",
    "    validation_data=valid_data,\n",
    "    validation_freq=1,\n",
    "    callbacks=[\n",
    "        tfk.callbacks.TensorBoard(log_dir=log_dir), \n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(default_logdir, \"params/%s/checkpoint\"%modelname),\n",
    "            save_weights_only=True,\n",
    "            monitor=\"val_recall\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True\n",
    "        ),\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor=\"val_recall\",\n",
    "            min_delta=1e-4,\n",
    "            patience=3,\n",
    "            mode=\"max\",\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfa024c-b9fe-4eae-9fa6-2a4956ca4851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f931c2bd550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SQNGRU4Rec(\n",
    "    num_items, \n",
    "    seq_len, \n",
    "    hidden_dim, \n",
    "    emb_dim, \n",
    "    dropout_rate=0.1, gamma=0.5)\n",
    "model.load_weights(\"/home/inoue/work/recs/params/diginetica/SQNGRU/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9dcb9d1-d8c0-45ba-9c59-416895dfb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open(\n",
    "    \"/home/inoue/work/dataset/%s/derived/mdp_test.df\"%dataname, \"rb\"))\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (test[0],test[1],test[2])).shuffle(len(test[0])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceecaa16-c0a3-45a5-a5ec-42bf84d0cae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcd177c7b974015961bd330f73105df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k =20\n",
    "df = pd.DataFrame(columns=[\"sessionId\", \"recIds\", \"choiceId\"])\n",
    "for batch in tqdm(test_data):\n",
    "    sess, state, target = batch\n",
    "    pred_score = model(state)\n",
    "    topkitem = tf.math.top_k(pred_score, k=k)[1].numpy() + 1\n",
    "    tmp = pd.DataFrame(\n",
    "        [sess.numpy(), topkitem, target.numpy()]).T\n",
    "    tmp.columns = [\"sessionId\", \"recIds\", \"choiceId\"]\n",
    "    df = pd.concat([df, tmp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccebadb-9b8f-4cf3-9fbb-37cb7e2f66f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k_ in [5, 10, 15, 20]:\n",
    "    df[\"NDCG@%d\"%k_] = df[[\"recIds\", \"choiceId\"]].apply(lambda x: metrics.ndcg_at_k(x[1], x[0], k=k_), axis=1)\n",
    "    df[\"Hit@%d\"%k_] = df[[\"recIds\", \"choiceId\"]].apply(lambda x: metrics.hit_at_k(x[1], x[0], k=k_), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a44dcdee-5c47-4d73-b7b5-117a40771185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NDCG@5     0.080413\n",
       "Hit@5      0.085838\n",
       "NDCG@10    0.102534\n",
       "Hit@10     0.133567\n",
       "NDCG@15    0.115500\n",
       "Hit@15     0.167548\n",
       "NDCG@20    0.124714\n",
       "Hit@20     0.194600\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sessionId\").mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a90431-5c30-468d-9e5c-3397b0f260f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GRU4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bdc5ddc-1c73-4452-988a-56cb1d7dbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU4Rec(tfk.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_items:int,\n",
    "        seq_len:Optional[int]=3,\n",
    "        hidden_dim:Optional[int]=100,\n",
    "        embed_dim:Optional[int]=100,\n",
    "        dropout_rate:Optional[float]=0.5,\n",
    "        k:Optional[int]=20,\n",
    "        name=\"GRU\"\n",
    "    ):\n",
    "        super(GRU4Rec, self).__init__(name=name)\n",
    "        self._topk = k\n",
    "        self._num_items = num_items\n",
    "        self._embedding = tfk.layers.Embedding(num_items, embed_dim, mask_zero=True)\n",
    "        self._gru = tfk.layers.GRU(\n",
    "            hidden_dim, \n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self._dense = tfk.layers.Dense(num_items, activation=\"softmax\")\n",
    "        self._recall_tracker = tfk.metrics.Recall(name=\"recall\")\n",
    "        \n",
    "    def call(\n",
    "        self, \n",
    "        item_seqs:tf.Tensor,\n",
    "        training:Optional[bool]=False\n",
    "    ):\n",
    "        \n",
    "        x = self._embedding(item_seqs)\n",
    "        x = self._gru(x, training=training)\n",
    "        out = self._dense(x)\n",
    "        return out\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        state, target = data\n",
    "        target = tf.one_hot(target, depth=self._num_items)\n",
    "        target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "        qvalue = self(state)\n",
    "        topkitem = tf.math.top_k(qvalue, k=self._topk)[1]\n",
    "        topkitem = tf.reduce_sum(tf.one_hot(topkitem, depth=self._num_items), axis=1)\n",
    "        topkitem = tf.cast(topkitem, dtype=tf.int32)\n",
    "        \n",
    "        self._recall_tracker.update_state(target, topkitem)\n",
    "        \n",
    "        return {\"recall\":self._recall_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d96291-922b-471c-b2e9-4173aa842b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname=\"diginetica\"\n",
    "modelname = \"GRU4Rec\"\n",
    "default_logdir = \"/home/inoue/work/recs/\"\n",
    "log_dir =  os.path.join(default_logdir, \"logs/%s/%s/\"%(dataname, modelname)+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "train = pickle.load(open(\n",
    "    \"/home/inoue/work/dataset/%s/derived/mdp_train.df\"%dataname, \"rb\"\n",
    "))\n",
    "\n",
    "split_ind = int(len(train[0])*0.9)\n",
    "data = pd.read_pickle(\"~/work/dataset/%s/derived/train.df\"%dataname)\n",
    "testdata = pd.read_pickle(\"~/work/dataset/%s/derived/test.df\"%dataname)\n",
    "\n",
    "num_items = max(data.itemId.max(), testdata.itemId.max())+1\n",
    "emb_dim = 64\n",
    "hidden_dim = 64\n",
    "seq_len = train[1].shape[1]\n",
    "batch_size=500\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (train[1][:split_ind, :],\n",
    "     train[2][:split_ind]-1)).shuffle(len(train[0][:split_ind])).batch(batch_size)\n",
    "valid_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (train[1][split_ind:, :],\n",
    "     train[2][split_ind:]-1)\n",
    ").shuffle(len(train[0][split_ind:])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f8d3d1-98bc-44b4-93f1-5f9799d5d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU4Rec(num_items, seq_len, hidden_dim, emb_dim, dropout_rate=0.1)\n",
    "model.compile(\n",
    "    loss=tfk.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tfk.optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "# model.build(input_shape=(1,seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b582c594-f4ed-49d2-95f0-a69e5d22f0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "646/646 [==============================] - 40s 55ms/step - loss: 9.0971 - recall: 0.0000e+00 - val_recall: 0.2430\n",
      "Epoch 2/100\n",
      "646/646 [==============================] - 34s 51ms/step - loss: 6.7215 - recall: 0.0000e+00 - val_recall: 0.3374\n",
      "Epoch 3/100\n",
      "646/646 [==============================] - 35s 53ms/step - loss: 5.4367 - recall: 0.0000e+00 - val_recall: 0.3508\n",
      "Epoch 4/100\n",
      "646/646 [==============================] - 34s 52ms/step - loss: 4.6938 - recall: 0.0000e+00 - val_recall: 0.3441\n",
      "Epoch 5/100\n",
      "646/646 [==============================] - 34s 52ms/step - loss: 4.2110 - recall: 0.0000e+00 - val_recall: 0.3393\n",
      "Epoch 6/100\n",
      "646/646 [==============================] - 34s 52ms/step - loss: 3.8773 - recall: 0.0000e+00 - val_recall: 0.3337\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f901c552910>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data, \n",
    "    epochs=100, \n",
    "    validation_data=valid_data,\n",
    "    validation_freq=1,\n",
    "    callbacks=[\n",
    "        tfk.callbacks.TensorBoard(log_dir=log_dir), \n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(default_logdir, \"params/%s/checkpoint\"%modelname),\n",
    "            save_weights_only=True,\n",
    "            monitor=\"val_recall\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True\n",
    "        ),\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor=\"val_recall\",\n",
    "            min_delta=1e-4,\n",
    "            patience=3,\n",
    "            mode=\"max\",\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d2b5a8-7a4a-4d21-87e1-0a26586ecedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open(\n",
    "    \"/home/inoue/work/dataset/%s/derived/mdp_test.df\"%dataname, \"rb\"))\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (test[0],test[1],test[2]-1)).shuffle(len(test[0])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "773c4756-5f02-4755-92d0-9363826ec5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d599f141246480c9a5e1f8ea11591e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k =20\n",
    "df = pd.DataFrame(columns=[\"sessionId\", \"recIds\", \"choiceId\"])\n",
    "for batch in tqdm(test_data):\n",
    "    sess, state, target = batch\n",
    "    pred_score = model(state)\n",
    "    topkitem = tf.math.top_k(pred_score, k=k)[1].numpy()\n",
    "    tmp = pd.DataFrame(\n",
    "        [sess.numpy(), topkitem, target.numpy()]).T\n",
    "    tmp.columns = [\"sessionId\", \"recIds\", \"choiceId\"]\n",
    "    df = pd.concat([df, tmp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a80eaa-d70a-4945-9c98-0e929b3a2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_ in [5, 10, 15, 20]:\n",
    "    df[\"NDCG@%d\"%k_] = df[[\"recIds\", \"choiceId\"]].apply(lambda x: metrics.ndcg_at_k(x[1], x[0], k=k_), axis=1)\n",
    "    df[\"Hit@%d\"%k_] = df[[\"recIds\", \"choiceId\"]].apply(lambda x: metrics.hit_at_k(x[1], x[0], k=k_), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7fa32f-8a28-49ed-aa4a-f0f2c64a357a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NDCG@5     0.088070\n",
       "Hit@5      0.093856\n",
       "NDCG@10    0.112826\n",
       "Hit@10     0.147244\n",
       "NDCG@15    0.127746\n",
       "Hit@15     0.186368\n",
       "NDCG@20    0.138285\n",
       "Hit@20     0.217302\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sessionId\").mean().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
